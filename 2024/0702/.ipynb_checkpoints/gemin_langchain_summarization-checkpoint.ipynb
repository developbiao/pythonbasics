{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a676c4c3-f802-4281-9c08-5635397b6647",
   "metadata": {},
   "source": [
    "## Summarize large documents using LangChain and Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548cb84c-dda6-4ba7-a6a6-d38fb6b33766",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, you must install the packages and set the necessary environemnt variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3ea91-5c4a-4f0d-8bc6-dbee76167d75",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install LangChin's Python library, langchin-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe9b65a-2c74-4128-ac40-5b0f239b25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ac052-d2ec-4644-a951-dff130032f2a",
   "metadata": {},
   "source": [
    "install LangChain's integration package for Gemini, langchin-google-genai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3717c838-9f17-48b8-8631-5dbcf6b13536",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7277d-920f-4f66-ac51-c104015b5971",
   "metadata": {},
   "source": [
    "### Grab an Cerdentials\n",
    "For example You will set the environment variable GOOGLE_APPLICATION_CREDENTIALS to configure vertex AI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88871aad-f69b-40fa-8ece-6df13321c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the config load success\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "import os\n",
    "import IPython\n",
    "from vertexai import generative_models\n",
    "from vertexai.generative_models import GenerativeModel, ChatSession\n",
    "# load google access config file\n",
    "credential_path=\"/Users/gongbiao/Code/vertex-ai/config/google_access_token_cp.json\"\n",
    "if os.path.exists(credential_path):\n",
    "    print(f\"the config load success\")\n",
    "else:\n",
    "    print(\"config file does'not exists!\")\n",
    "    \n",
    "# init vertex ai\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
    "project_id = \"gen-lang-client-0115788367\"\n",
    "location = \"us-central1\"\n",
    "vertexai.init(project=project_id, location=location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d446da6-56d2-4c1a-88bc-1ee664e8a485",
   "metadata": {},
   "source": [
    "### Setup proxy (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbaa6851-e001-4f89-a4fe-f7756a7f3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optinal] set proxy\n",
    "proxy = \"http://127.0.0.1:8889\"\n",
    "os.environ[\"HTTP_PROXY\"] = proxy\n",
    "os.environ[\"HTTPS_PROXY\"] = proxy\n",
    "os.environ[\"http_proxy\"] = proxy\n",
    "os.environ[\"https_proxy\"] = proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1213cb6-1975-41f8-96ac-9d58508ccf48",
   "metadata": {},
   "source": [
    "### Summarize text\n",
    "In this tutorial, you are going to summarize the text from a website using the Gemin mode integrated through LangChian.\n",
    "You'll perform the following steps to archieve the same:\n",
    "1.  Read and parse the website data using LangChain.\n",
    "2. Chain together the following:\n",
    "- A prompt for extracting the required input data from the parsed website data\n",
    "- A prompt from summarizing the text using LangChain.\n",
    "- An LLM model (Gemini) forprompting.\n",
    "3. Run the cretead chain to prompt the model for the summary of the website data.\n",
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e06a01ac-1bef-4655-aace-449c21f8d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373918a1-7c41-4441-bdab-4f96ea1f5cfe",
   "metadata": {},
   "source": [
    "### read and parse the website data\n",
    "LangChain provides a wide variety of document loaders. To read the website data as a document, you will use the WebBaseLoader from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a65ee03-539a-4f43-899f-b8267f2766e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://coolshell.cn/articles/20793.html\")\n",
    "docs = loader.load()\n",
    "#print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d680c0-5f0d-445b-8084-9bdcac81a617",
   "metadata": {},
   "source": [
    "### Initialize Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c8dca66-3bd2-47cc-8025-882ae221b2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I don't experience emotions like humans do.  However, I'm ready to assist you with any questions or tasks you may have! 😊  What can I help you with today? \\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "llm = VertexAI(model_name=\"gemini-1.5-pro-001\", temperature=0.7, top_p=0.85)\n",
    "llm.invoke(\"How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3f79b7d-6e1d-4d5f-8a93-d7b9e6c0b5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] template='Use chinese Write a concise summary of the following:\\n\"{text}\"\\nCONCISE SUMMARY:'\n"
     ]
    }
   ],
   "source": [
    "# To extract data from WebBaseLoader\n",
    "doc_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
    "\n",
    "# To query Gemini\n",
    "llm_prompt_template = \"\"\"Use chinese Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af56590-68e6-4167-9fd4-2fbd25e6c637",
   "metadata": {},
   "source": [
    "### Create a Stuff documents chain\n",
    "LangChain provides Chains for chaining together LLMs with each other or other components for complex applications. \n",
    "You will create a <b>Stuff documents chain</b> for this application. A <b>Stuff documents chain</b> lets you combine all the documents, insert them into the prompt and pass that prompt to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2ccaf19-fa59-43d5-b712-55b99a1b0940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chain implements the folowing pipeline:\n",
    "# 1. Extract data from documents and save to variable `text`.\n",
    "# 2. This `text` is then passed to the prompt and input variable in prompt is populated.\n",
    "# 3. The prompt is then passed to the LLM (Gemini).\n",
    "# 4. Output from the LLM is passed through an output parser to structure the model response.\n",
    "\n",
    "stuff_chain = (\n",
    "    # Extract data from the documents and add to the key `text`.\n",
    "    {\n",
    "        \"text\": lambda docs: \"\\n\\n\".join(\n",
    "            format_document(doc, doc_prompt) for doc in docs\n",
    "        )\n",
    "    }\n",
    "    | llm_prompt         # Prompt for Gemini\n",
    "    | llm                # Gemini function\n",
    "    | StrOutputParser()  # output parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c7b66-6d50-478f-a171-44dfe3df7fe3",
   "metadata": {},
   "source": [
    "### Prompt the model\n",
    "To generate the summary of the website data, pass the documents extracted using the `WebBaseLoader` (`docs`) to `invoke()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47129ed5-1272-482b-946d-f44bbde0b19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这篇文章讲解了与程序员相关的CPU缓存知识。文章首先介绍了CPU缓存的基本概念，包括缓存的层级结构、大小、速度以及缓存行的概念。接着，文章解释了缓存命中的重要性，并通过代码示例说明了缓存命中率对程序性能的影响。\\n\\n随后，文章深入探讨了多核CPU下的缓存一致性问题，介绍了两种常见的缓存一致性协议：Directory协议和Snoopy协议，并详细解释了MESI和MOESI协议的工作原理。\\n\\n最后，文章通过五个代码示例，展示了缓存行、缓存命中率、缓存一致性以及'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a8125-2ab5-4f4a-b749-35814b290703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
