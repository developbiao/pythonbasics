{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a27f8a2-3109-4507-8215-5d8f5c6c3eb4",
   "metadata": {},
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f6aa77-0c87-4aab-bc40-3fb5deec84e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy_value: http://localhost:7897\n"
     ]
    }
   ],
   "source": [
    "### Prepare environment\n",
    "import os, getpass\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}:\")\n",
    "# set proxy\n",
    "_set_env(\"PROXY_VALUE\")\n",
    "proxy = os.getenv(\"PROXY_VALUE\")\n",
    "for proxy_var in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:\n",
    "    os.environ[proxy_var] = proxy\n",
    "# GOOGLE API KEY\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "print(\"proxy_value:\", proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c737ea-c73a-466b-b315-edf7b6f9d8dc",
   "metadata": {},
   "source": [
    "### Set langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e674ba49-df73-4671-bd28-c0d9996b8cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccea303-5547-4a90-a2d3-19dfdef19c1d",
   "metadata": {},
   "source": [
    "### Agent\n",
    "In the context of agents, waiting for user feedback is especially useful for asking clarifying qeustions.\n",
    "To illustrate this, we'll create a simple ReAct-style agent capable of tool calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f29e366-f2ec-4ccd-a495-21adebd4b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the state\n",
    "from langgraph.graph import MessagesState, START\n",
    "\n",
    "# Set up the tool\n",
    "# We will have one real tool -a search tool\n",
    "# We'll also have one \"fake\" tool -a \"ask_human\" tool\n",
    "# Here we define any ACTUAL tools\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"\n",
    "    Call to surf the web.\n",
    "    \"\"\"\n",
    "    # This is a placehodler for the acutal implementation\n",
    "    # Don't let the LLM know this though\n",
    "    return f\"I looked up: {query}. Result: It's sunny in Chengdu, but you better look out if your're a Gemini.\"\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Set up the Gemini model\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=512,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# We are going \"bind\" call tools to the model\n",
    "# We have the ACTUAL tools from above, but we also need a mock tool to ask a human\n",
    "# Since `bind_tools` takes in tools but also just tool definitions\n",
    "# We can define a tool definition for `ask_human`\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask the human a question\"\"\"\n",
    "    question: str\n",
    "\n",
    "model = model.bind_tools(tools + [AskHuman])\n",
    "\n",
    "# Define node and conditonal edges\n",
    "\n",
    "# Define the function that determines whether to contine or not\n",
    "def should_contine(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "\n",
    "    # If tool call is asking Human, we return that node\n",
    "    # You could also add logic here to let some system know that there's something that requires Human input\n",
    "    # For example, send a slack message, etc\n",
    "    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n",
    "        return \"ask_human\"\n",
    "    # Otherwise if there is, we contine\n",
    "    else:\n",
    "        return \"action\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f962a8e-e628-4c3a-890d-ed6bf109984e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd24828-3994-4cac-84de-d24d1efaf153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
